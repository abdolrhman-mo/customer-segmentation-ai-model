{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3d0f9a",
   "metadata": {},
   "source": [
    "**Phases 8-9: evaluation, and optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306db8",
   "metadata": {},
   "source": [
    "## **Phase 8: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23eee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3a86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_scaled = joblib.load(\"../data/processed/X_train_scaled.pkl\")\n",
    "y_train = joblib.load(\"../data/processed/y_train.pkl\")\n",
    "X_test_scaled = joblib.load(\"../data/processed/X_test_scaled.pkl\")\n",
    "y_test = joblib.load(\"../data/processed/y_test.pkl\")\n",
    "\n",
    "lr_model = joblib.load(\"../models/logistic_regression.pkl\")\n",
    "svm_model = joblib.load(\"../models/svm.pkl\")\n",
    "rf_model = joblib.load(\"../models/random_forest.pkl\")\n",
    "xgb_model = joblib.load(\"../models/xgboost.pkl\")\n",
    "voting_model = joblib.load(\"../models/voting_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb646571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_voting = voting_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1a0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        72\n",
      "           1       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "SVM\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "XGBOOST\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "VOTING CLASSIFIER\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for cleaner code\n",
    "def evaluate_model(model, y_true, y_pred, model_name, X_test_data):\n",
    "    \"\"\"Evaluate a single model and return metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each model\n",
    "lr_results = evaluate_model(lr_model, y_test, y_pred_lr, \"Logistic Regression\", X_test_scaled)\n",
    "svm_results = evaluate_model(svm_model, y_test, y_pred_svm, \"SVM\", X_test_scaled)\n",
    "xgb_results = evaluate_model(xgb_model, y_test, y_pred_xgb, \"XGBoost\", X_test_scaled)\n",
    "rf_results = evaluate_model(rf_model, y_test, y_pred_rf, \"Random Forest\", X_test_scaled)\n",
    "voting_results = evaluate_model(voting_model, y_test, y_pred_voting, \"Voting Classifier\", X_test_scaled)\n",
    "\n",
    "# Display detailed results for each model\n",
    "models_to_evaluate = [lr_results, svm_results, xgb_results, rf_results, voting_results]\n",
    "\n",
    "for model_result in models_to_evaluate:\n",
    "    print(f\"\\n{model_result['Model'].upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Recall:    {model_result['Recall']:.4f}\")\n",
    "    # print(f\"Accuracy:  {model_result['Accuracy']:.4f}\")\n",
    "    # print(f\"Precision: {model_result['Precision']:.4f}\")\n",
    "    # print(f\"F1 Score:  {model_result['F1 Score']:.4f}\")\n",
    "    # print(f\"ROC AUC:   {model_result['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Get predictions for classification report\n",
    "    if model_result['Model'] == \"Logistic Regression\":\n",
    "        y_pred_for_report = y_pred_lr\n",
    "    elif model_result['Model'] == \"SVM\":\n",
    "        y_pred_for_report = y_pred_svm\n",
    "    elif model_result['Model'] == \"XGBoost\":\n",
    "        y_pred_for_report = y_pred_xgb\n",
    "    else:  # Random Forest\n",
    "        y_pred_for_report = y_pred_rf\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_for_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6aa70",
   "metadata": {},
   "source": [
    "## **Phase 9: Model Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93863aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ SVM THRESHOLD OPTIMIZATION - FINDING BEST RECALL\n",
      "============================================================\n",
      "Threshold | Recall  | Precision | F1-Score\n",
      "---------------------------------------------\n",
      "   0.1    | 0.9762  | 0.8200    | 0.8913\n",
      "   0.2    | 0.9762  | 0.9762    | 0.9762\n",
      "   0.3    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.4    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.5    | 0.9286  | 1.0000    | 0.9630\n",
      "   0.6    | 0.8810  | 1.0000    | 0.9367\n",
      "   0.7    | 0.8333  | 1.0000    | 0.9091\n",
      "   0.8    | 0.7857  | 1.0000    | 0.8800\n",
      "   0.9    | 0.7619  | 1.0000    | 0.8649\n",
      "\n",
      "ðŸ† BEST SVM THRESHOLD: 0.1\n",
      "   Recall: 0.9762\n",
      "   Precision: 0.8200\n",
      "   F1-Score: 0.8913\n",
      "\n",
      "ðŸ”´ HIGH-RISK CUSTOMERS (Probability >= 0.1): 50\n",
      "âœ… SVM threshold optimization completed!\n",
      "ðŸ’¡ Use threshold 0.1 for production deployment\n"
     ]
    }
   ],
   "source": [
    "# Simple SVM threshold optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ SVM THRESHOLD OPTIMIZATION - FINDING BEST RECALL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get SVM probabilities (using the same data format as your existing code)\n",
    "voting_probs = voting_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Test different thresholds and find best recall\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_recall = 0\n",
    "best_threshold = 0.5\n",
    "best_metrics = {}\n",
    "\n",
    "print(\"Threshold | Recall  | Precision | F1-Score\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    predictions = (voting_probs >= threshold).astype(int)\n",
    "    pred_labels = [1 if pred == 1 else 0 for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = recall_score(y_test, pred_labels, pos_label=1)\n",
    "    precision = precision_score(y_test, pred_labels, pos_label=1)\n",
    "    f1 = f1_score(y_test, pred_labels, pos_label=1)\n",
    "    \n",
    "    print(f\"{threshold:^9} | {recall:.4f}  | {precision:.4f}    | {f1:.4f}\")\n",
    "    \n",
    "    # Track best recall\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {'recall': recall, 'precision': precision, 'f1': f1}\n",
    "\n",
    "print(f\"\\nðŸ† BEST SVM THRESHOLD: {best_threshold}\")\n",
    "print(f\"   Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"   Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"   F1-Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# Show high-risk customers\n",
    "high_risk_count = (svm_probs >= best_threshold).sum()\n",
    "print(f\"\\nðŸ”´ HIGH-RISK CUSTOMERS (Probability >= {best_threshold}): {high_risk_count}\")\n",
    "\n",
    "print(\"âœ… SVM threshold optimization completed!\")\n",
    "print(f\"ðŸ’¡ Use threshold {best_threshold} for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac233f8",
   "metadata": {},
   "source": [
    "## Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f568bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ’¾ SAVING OPTIMIZED VOTING MODEL\n",
      "============================================================\n",
      "âœ… Best model saved to: ../models/best_model.pkl\n",
      "âœ… Best threshold saved to: ../models/best_threshold.txt\n",
      "âœ… Scaler saved to: ../models/scaler.pkl\n",
      "\n",
      "ðŸŽ¯ Model deployment ready!\n",
      "   - Use threshold: 0.1\n",
      "   - Model file: ../models/best_model.pkl\n",
      "   - For new predictions: load model and apply threshold 0.1\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save the optimized SVM model with best threshold\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¾ SAVING OPTIMIZED VOTING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_path = \"../models\"\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save the Voting model\n",
    "best_model_path = f\"{models_path}/best_model.pkl\"\n",
    "joblib.dump(voting_model, best_model_path)\n",
    "print(f\"âœ… Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save the best threshold value\n",
    "threshold_path = f\"{models_path}/best_threshold.txt\"\n",
    "with open(threshold_path, 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "print(f\"âœ… Best threshold saved to: {threshold_path}\")\n",
    "\n",
    "# Save the scaler for preprocessing new data\n",
    "scaler = StandardScaler()\n",
    "scaler_path = f\"{models_path}/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model deployment ready!\")\n",
    "print(f\"   - Use threshold: {best_threshold}\")\n",
    "print(f\"   - Model file: {best_model_path}\")\n",
    "print(f\"   - For new predictions: load model and apply threshold {best_threshold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
