{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3d0f9a",
   "metadata": {},
   "source": [
    "**Phases 8-9: evaluation, and optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306db8",
   "metadata": {},
   "source": [
    "## **Phase 8: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23eee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3a86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_scaled = joblib.load(\"../data/processed/X_train_scaled.pkl\")\n",
    "y_train = joblib.load(\"../data/processed/y_train.pkl\")\n",
    "X_test_scaled = joblib.load(\"../data/processed/X_test_scaled.pkl\")\n",
    "y_test = joblib.load(\"../data/processed/y_test.pkl\")\n",
    "\n",
    "lr_model = joblib.load(\"../models/logistic_regression.pkl\")\n",
    "svm_model = joblib.load(\"../models/svm.pkl\")\n",
    "rf_model = joblib.load(\"../models/random_forest.pkl\")\n",
    "xgb_model = joblib.load(\"../models/xgboost.pkl\")\n",
    "voting_model = joblib.load(\"../models/voting_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb646571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_voting = voting_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1a0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        72\n",
      "           1       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "SVM\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "XGBOOST\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "VOTING CLASSIFIER\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for cleaner code\n",
    "def evaluate_model(model, y_true, y_pred, model_name, X_test_data):\n",
    "    \"\"\"Evaluate a single model and return metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each model\n",
    "lr_results = evaluate_model(lr_model, y_test, y_pred_lr, \"Logistic Regression\", X_test_scaled)\n",
    "svm_results = evaluate_model(svm_model, y_test, y_pred_svm, \"SVM\", X_test_scaled)\n",
    "xgb_results = evaluate_model(xgb_model, y_test, y_pred_xgb, \"XGBoost\", X_test_scaled)\n",
    "rf_results = evaluate_model(rf_model, y_test, y_pred_rf, \"Random Forest\", X_test_scaled)\n",
    "voting_results = evaluate_model(voting_model, y_test, y_pred_voting, \"Voting Classifier\", X_test_scaled)\n",
    "\n",
    "# Display detailed results for each model\n",
    "models_to_evaluate = [lr_results, svm_results, xgb_results, rf_results, voting_results]\n",
    "\n",
    "for model_result in models_to_evaluate:\n",
    "    print(f\"\\n{model_result['Model'].upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Recall:    {model_result['Recall']:.4f}\")\n",
    "    # print(f\"Accuracy:  {model_result['Accuracy']:.4f}\")\n",
    "    # print(f\"Precision: {model_result['Precision']:.4f}\")\n",
    "    # print(f\"F1 Score:  {model_result['F1 Score']:.4f}\")\n",
    "    # print(f\"ROC AUC:   {model_result['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Get predictions for classification report\n",
    "    if model_result['Model'] == \"Logistic Regression\":\n",
    "        y_pred_for_report = y_pred_lr\n",
    "    elif model_result['Model'] == \"SVM\":\n",
    "        y_pred_for_report = y_pred_svm\n",
    "    elif model_result['Model'] == \"XGBoost\":\n",
    "        y_pred_for_report = y_pred_xgb\n",
    "    else:  # Random Forest\n",
    "        y_pred_for_report = y_pred_rf\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_for_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6aa70",
   "metadata": {},
   "source": [
    "## **Phase 9: Model Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93863aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ SVM THRESHOLD OPTIMIZATION - FINDING BEST RECALL\n",
      "============================================================\n",
      "Threshold | Recall  | Precision | F1-Score\n",
      "---------------------------------------------\n",
      "   0.1    | 0.9762  | 0.8200    | 0.8913\n",
      "   0.2    | 0.9762  | 0.9762    | 0.9762\n",
      "   0.3    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.4    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.5    | 0.9286  | 1.0000    | 0.9630\n",
      "   0.6    | 0.8810  | 1.0000    | 0.9367\n",
      "   0.7    | 0.8333  | 1.0000    | 0.9091\n",
      "   0.8    | 0.7857  | 1.0000    | 0.8800\n",
      "   0.9    | 0.7619  | 1.0000    | 0.8649\n",
      "\n",
      "ðŸ† BEST SVM THRESHOLD: 0.1\n",
      "   Recall: 0.9762\n",
      "   Precision: 0.8200\n",
      "   F1-Score: 0.8913\n",
      "\n",
      "ðŸ”´ HIGH-RISK CUSTOMERS (Probability >= 0.1): 50\n",
      "âœ… SVM threshold optimization completed!\n",
      "ðŸ’¡ Use threshold 0.1 for production deployment\n"
     ]
    }
   ],
   "source": [
    "# Simple SVM threshold optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ Voting Model THRESHOLD OPTIMIZATION - FINDING BEST RECALL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get SVM probabilities (using the same data format as your existing code)\n",
    "voting_probs = voting_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Test different thresholds and find best recall\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_recall = 0\n",
    "best_threshold = 0.5\n",
    "best_metrics = {}\n",
    "\n",
    "print(\"Threshold | Recall  | Precision | F1-Score\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    predictions = (voting_probs >= threshold).astype(int)\n",
    "    pred_labels = [1 if pred == 1 else 0 for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = recall_score(y_test, pred_labels, pos_label=1)\n",
    "    precision = precision_score(y_test, pred_labels, pos_label=1)\n",
    "    f1 = f1_score(y_test, pred_labels, pos_label=1)\n",
    "    \n",
    "    print(f\"{threshold:^9} | {recall:.4f}  | {precision:.4f}    | {f1:.4f}\")\n",
    "    \n",
    "    # Track best recall\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {'recall': recall, 'precision': precision, 'f1': f1}\n",
    "\n",
    "print(f\"\\nðŸ† BEST Voting Model THRESHOLD: {best_threshold}\")\n",
    "print(f\"   Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"   Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"   F1-Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# Show high-risk customers\n",
    "high_risk_count = (svm_probs >= best_threshold).sum()\n",
    "print(f\"\\nðŸ”´ HIGH-RISK PATIENTS (Probability >= {best_threshold}): {high_risk_count}\")\n",
    "\n",
    "print(\"âœ… Voting Model threshold optimization completed!\")\n",
    "print(f\"ðŸ’¡ Use threshold {best_threshold} for production deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f5d9f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics_path\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Use this after you find your best threshold in your notebook:\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# save_performance_metrics(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#     algorithm=\"SVM\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43msave_performance_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecision\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVoting Model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msave_performance_metrics\u001b[39m\u001b[34m(best_threshold, recall, precision, f1_score, algorithm)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mSave the best model's performance metrics to a JSON file\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m- algorithm: Model algorithm name\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create metrics dictionary\u001b[39;00m\n\u001b[32m     17\u001b[39m metrics = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33malgorithm\u001b[39m\u001b[33m\"\u001b[39m: algorithm,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_threshold\u001b[39m\u001b[33m\"\u001b[39m: best_threshold,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcore_metrics\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(recall, \u001b[32m4\u001b[39m),\n\u001b[32m     22\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(precision, \u001b[32m4\u001b[39m),\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(f1_score, \u001b[32m4\u001b[39m)\n\u001b[32m     24\u001b[39m     },\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraining_info\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m: pd.Timestamp.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_train\u001b[49m) + \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[32m     28\u001b[39m     }\n\u001b[32m     29\u001b[39m }\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Create models directory if it doesn't exist\u001b[39;00m\n\u001b[32m     32\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_performance_metrics(best_threshold, recall, precision, f1_score, algorithm=\"Voting Model\"):\n",
    "    \"\"\"\n",
    "    Save the best model's performance metrics to a JSON file\n",
    "    \n",
    "    Parameters:\n",
    "    - best_threshold: Optimal threshold value\n",
    "    - recall, precision, f1_score, accuracy: Core metrics\n",
    "    - specificity, roc_auc: Optional additional metrics\n",
    "    - algorithm: Model algorithm name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"algorithm\": algorithm,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"core_metrics\": {\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"f1_score\": round(f1_score, 4)\n",
    "        },\n",
    "        \"training_info\": {\n",
    "            \"date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"dataset_size\": len(X_train) + len(X_test)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    metrics_path = \"models/performance_metrics.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Performance metrics saved to: {metrics_path}\")\n",
    "    return metrics_path\n",
    "\n",
    "# Use this after you find your best threshold in your notebook:\n",
    "# Example usage:\n",
    "# save_performance_metrics(\n",
    "#     best_threshold=0.1,\n",
    "#     recall=0.9762,\n",
    "#     precision=0.8200,\n",
    "#     f1_score=0.8913,\n",
    "#     accuracy=0.8947,\n",
    "#     specificity=0.8235,\n",
    "#     roc_auc=0.9234,\n",
    "#     algorithm=\"SVM\"\n",
    "# )\n",
    "save_performance_metrics(\n",
    "    best_threshold=best_threshold,\n",
    "    recall=best_metrics['recall'],\n",
    "    precision=best_metrics['precision'],\n",
    "    f1_score=best_metrics['f1'],\n",
    "    algorithm=\"Voting Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac233f8",
   "metadata": {},
   "source": [
    "## Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f568bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ’¾ SAVING OPTIMIZED VOTING MODEL\n",
      "============================================================\n",
      "âœ… Best model saved to: ../models/best_model.pkl\n",
      "âœ… Best threshold saved to: ../models/best_threshold.txt\n",
      "âœ… Scaler saved to: ../models/scaler.pkl\n",
      "\n",
      "ðŸŽ¯ Model deployment ready!\n",
      "   - Use threshold: 0.1\n",
      "   - Model file: ../models/best_model.pkl\n",
      "   - For new predictions: load model and apply threshold 0.1\n"
     ]
    }
   ],
   "source": [
    "# Save the optimized SVM model with best threshold\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¾ SAVING OPTIMIZED VOTING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_path = \"../models\"\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save the Voting model\n",
    "best_model_path = f\"{models_path}/best_model.pkl\"\n",
    "joblib.dump(voting_model, best_model_path)\n",
    "print(f\"âœ… Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save the best threshold value\n",
    "threshold_path = f\"{models_path}/best_threshold.txt\"\n",
    "with open(threshold_path, 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "print(f\"âœ… Best threshold saved to: {threshold_path}\")\n",
    "\n",
    "# Save the scaler for preprocessing new data\n",
    "scaler = StandardScaler()\n",
    "scaler_path = f\"{models_path}/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model deployment ready!\")\n",
    "print(f\"   - Use threshold: {best_threshold}\")\n",
    "print(f\"   - Model file: {best_model_path}\")\n",
    "print(f\"   - For new predictions: load model and apply threshold {best_threshold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
